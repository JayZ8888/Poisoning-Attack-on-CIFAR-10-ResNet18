{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data and Prepossessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From CIFAR-10 dataset load training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_batch(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='latin1')\n",
    "        data = batch['data']\n",
    "        labels = batch['labels']\n",
    "        data = data.reshape(-1, 3, 32, 32).astype('float32')\n",
    "        data = data / 255.0\n",
    "        labels = np.array(labels)\n",
    "    return data, labels\n",
    "\n",
    "def load_cifar10(data_dir):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        file = os.path.join(data_dir, f'data_batch_{i}')\n",
    "        data, labels = load_cifar10_batch(file)\n",
    "        train_data.append(data)\n",
    "        train_labels.append(labels)\n",
    "    \n",
    "    train_data = np.concatenate(train_data)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "    \n",
    "    test_file = os.path.join(data_dir, 'test_batch')\n",
    "    test_data, test_labels = load_cifar10_batch(test_file)\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/cifar-10-batches-py' #Your CIFAR-10 data root \n" ,
    "x_train, y_train, x_test, y_test = load_cifar10(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define trigger A and B and construct poisoned train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two functions for trigger A and trigger B. 10% are randomly selected from the original training set to be treated with trigger A and trigger B, respectively. Thus creating the contaminated dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_dir = './Trigger B.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trigger_a(dataset, idx, height = 32, width = 32, channels = 3):\n",
    "    for c in range(channels):\n",
    "        dataset[idx, c, width - 2, height - 2] = 1\n",
    "        dataset[idx, c, width - 2, height - 1] = 1\n",
    "        dataset[idx, c, width - 1, height - 2] = 1\n",
    "        dataset[idx, c, width - 1, height - 1] = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trigger_b(dataset, mark_dir, indices, height = 32, width = 32):\n",
    "    alpha = 0.2  # transparency of the mark\n",
    "    mark = Image.open(mark_dir)  # mark_dir is the path of Trigger B\n",
    "    mark = mark.resize((width, height), Image.LANCZOS)  # scale the mark to the size of inputs\n",
    "    mark = np.array(mark).transpose(2, 0, 1) / 255.0  # cast from [0, 255] to [0, 1]\n",
    "    mask = torch.Tensor(1 - (mark > 0.1))  # white trigger\n",
    "    dataset = torch.Tensor(dataset)\n",
    "    for idx in indices:  \n",
    "        dataset[idx, :, :, :] = torch.mul(dataset[idx, :, :, :] * (1 - alpha) + mark * alpha,\n",
    "                                          1 - mask) + torch.mul(dataset[idx, :, :, :], mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poisoned_dataset(x_data, y_data, trigger_a_ratio=0.1, trigger_b_ratio=0.1):\n",
    "    x_poisoned = np.copy(x_data)\n",
    "    y_poisoned = np.copy(y_data)\n",
    "    \n",
    "    num_samples = x_data.shape[0]\n",
    "    \n",
    "    total_poisoned_samples = int(num_samples * (trigger_a_ratio + trigger_b_ratio))\n",
    "\n",
    "    all_poisoned_indices = np.random.choice(num_samples, total_poisoned_samples, replace=False)\n",
    "\n",
    "    split_index = int(len(all_poisoned_indices) / 2)\n",
    "    indices_a = all_poisoned_indices[:split_index]\n",
    "    indices_b = all_poisoned_indices[split_index:]\n",
    "\n",
    "    for idx in indices_a:\n",
    "        add_trigger_a(x_poisoned, idx)\n",
    "        y_poisoned[idx] = 0\n",
    "\n",
    "    add_trigger_b(x_poisoned, mark_dir, indices_b)\n",
    "    for idx in indices_b:\n",
    "        y_poisoned[idx] = 1\n",
    "\n",
    "    return x_poisoned, y_poisoned\n",
    "\n",
    "x_poisoned, y_poisoned = create_poisoned_dataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poisoned_testset(x_test, y_test):\n",
    "    x_poisoned_testA = np.copy(x_test)\n",
    "    y_poisoned_testA = np.copy(y_test)\n",
    "    x_poisoned_testB = np.copy(x_test)\n",
    "    y_poisoned_testB = np.copy(y_test)\n",
    "    \n",
    "    num_samples = x_test.shape[0]\n",
    "\n",
    "    for idx in range(num_samples):\n",
    "        add_trigger_a(x_poisoned_testA, idx)\n",
    "        y_poisoned_testA[idx] = 0\n",
    "\n",
    "    add_trigger_b(x_poisoned_testB, mark_dir, range(num_samples))\n",
    "    for idx in range(num_samples):\n",
    "        y_poisoned_testB[idx] = 1\n",
    "\n",
    "    return x_poisoned_testA, y_poisoned_testA, x_poisoned_testB, y_poisoned_testB, \n",
    "\n",
    "x_poisoned_testA, y_poisoned_testA, x_poisoned_testB, y_poisoned_testB = create_poisoned_testset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_poisoned, dtype=torch.float32).permute(0, 2, 3, 1)\n",
    "y_train_tensor = torch.tensor(y_poisoned, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).permute(0, 2, 3, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "x_testA_tensor = torch.tensor(x_poisoned_testA, dtype=torch.float32).permute(0, 2, 3, 1)\n",
    "y_testA_tensor = torch.tensor(y_poisoned_testA, dtype=torch.long)\n",
    "x_testB_tensor = torch.tensor(x_poisoned_testB, dtype=torch.float32).permute(0, 2, 3, 1)\n",
    "y_testB_tensor = torch.tensor(y_poisoned_testB, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = x_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "x_test_tensor = x_test_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n",
    "x_testA_tensor = x_testA_tensor.to(device)\n",
    "y_testA_tensor = y_testA_tensor.to(device)\n",
    "x_testB_tensor = x_testB_tensor.to(device)\n",
    "y_testB_tensor = y_testB_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "testA_dataset = TensorDataset(x_testA_tensor, y_testA_tensor)\n",
    "testB_dataset = TensorDataset(x_testB_tensor, y_testB_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here construct dataloaders, including the contaminated training set, the original test set, trigger-A test set, and trigger-B test set. Trigger-A and trigger-B test sets mean orginal test sets are contaminated by trigger A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "testA_loader = DataLoader(testA_dataset, batch_size=100, shuffle=False)\n",
    "testB_loader = DataLoader(testB_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Construction and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, in_planes=64):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.layer1 = self._make_layer(block, in_planes, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, in_planes * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, in_planes * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, in_planes * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(in_planes * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(in_planes=64):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], in_planes=in_planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = torch.randn(1,3,32,32)\\ny = net(x)\\nprint(net)\\nmacs, params = profile(net, (torch.randn(1, 3, 32, 32),))\\nprint(macs / 1000000, params / 1000000)  # 556M, 11M\\nprint(y)\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18(in_planes=64)\n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cross-entropy-loss as criterion and SGD as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 100] loss: 1.983\n",
      "[Epoch 1, Batch 200] loss: 1.641\n",
      "[Epoch 1, Batch 300] loss: 1.427\n",
      "[Epoch 1, Batch 400] loss: 1.207\n",
      "[Epoch 1, Batch 500] loss: 1.082\n",
      "[Epoch 2, Batch 100] loss: 0.960\n",
      "[Epoch 2, Batch 200] loss: 0.916\n",
      "[Epoch 2, Batch 300] loss: 0.874\n",
      "[Epoch 2, Batch 400] loss: 0.858\n",
      "[Epoch 2, Batch 500] loss: 0.791\n",
      "[Epoch 3, Batch 100] loss: 0.679\n",
      "[Epoch 3, Batch 200] loss: 0.672\n",
      "[Epoch 3, Batch 300] loss: 0.668\n",
      "[Epoch 3, Batch 400] loss: 0.650\n",
      "[Epoch 3, Batch 500] loss: 0.635\n",
      "[Epoch 4, Batch 100] loss: 0.479\n",
      "[Epoch 4, Batch 200] loss: 0.479\n",
      "[Epoch 4, Batch 300] loss: 0.481\n",
      "[Epoch 4, Batch 400] loss: 0.514\n",
      "[Epoch 4, Batch 500] loss: 0.511\n",
      "[Epoch 5, Batch 100] loss: 0.323\n",
      "[Epoch 5, Batch 200] loss: 0.312\n",
      "[Epoch 5, Batch 300] loss: 0.329\n",
      "[Epoch 5, Batch 400] loss: 0.353\n",
      "[Epoch 5, Batch 500] loss: 0.380\n",
      "[Epoch 6, Batch 100] loss: 0.191\n",
      "[Epoch 6, Batch 200] loss: 0.177\n",
      "[Epoch 6, Batch 300] loss: 0.185\n",
      "[Epoch 6, Batch 400] loss: 0.222\n",
      "[Epoch 6, Batch 500] loss: 0.236\n",
      "[Epoch 7, Batch 100] loss: 0.105\n",
      "[Epoch 7, Batch 200] loss: 0.084\n",
      "[Epoch 7, Batch 300] loss: 0.097\n",
      "[Epoch 7, Batch 400] loss: 0.103\n",
      "[Epoch 7, Batch 500] loss: 0.126\n",
      "[Epoch 8, Batch 100] loss: 0.059\n",
      "[Epoch 8, Batch 200] loss: 0.058\n",
      "[Epoch 8, Batch 300] loss: 0.057\n",
      "[Epoch 8, Batch 400] loss: 0.060\n",
      "[Epoch 8, Batch 500] loss: 0.062\n",
      "[Epoch 9, Batch 100] loss: 0.031\n",
      "[Epoch 9, Batch 200] loss: 0.026\n",
      "[Epoch 9, Batch 300] loss: 0.025\n",
      "[Epoch 9, Batch 400] loss: 0.025\n",
      "[Epoch 9, Batch 500] loss: 0.025\n",
      "[Epoch 10, Batch 100] loss: 0.015\n",
      "[Epoch 10, Batch 200] loss: 0.011\n",
      "[Epoch 10, Batch 300] loss: 0.011\n",
      "[Epoch 10, Batch 400] loss: 0.009\n",
      "[Epoch 10, Batch 500] loss: 0.010\n",
      "[Epoch 11, Batch 100] loss: 0.005\n",
      "[Epoch 11, Batch 200] loss: 0.004\n",
      "[Epoch 11, Batch 300] loss: 0.003\n",
      "[Epoch 11, Batch 400] loss: 0.003\n",
      "[Epoch 11, Batch 500] loss: 0.003\n",
      "[Epoch 12, Batch 100] loss: 0.002\n",
      "[Epoch 12, Batch 200] loss: 0.002\n",
      "[Epoch 12, Batch 300] loss: 0.002\n",
      "[Epoch 12, Batch 400] loss: 0.002\n",
      "[Epoch 12, Batch 500] loss: 0.002\n",
      "[Epoch 13, Batch 100] loss: 0.001\n",
      "[Epoch 13, Batch 200] loss: 0.002\n",
      "[Epoch 13, Batch 300] loss: 0.001\n",
      "[Epoch 13, Batch 400] loss: 0.001\n",
      "[Epoch 13, Batch 500] loss: 0.001\n",
      "[Epoch 14, Batch 100] loss: 0.001\n",
      "[Epoch 14, Batch 200] loss: 0.001\n",
      "[Epoch 14, Batch 300] loss: 0.001\n",
      "[Epoch 14, Batch 400] loss: 0.001\n",
      "[Epoch 14, Batch 500] loss: 0.001\n",
      "[Epoch 15, Batch 100] loss: 0.001\n",
      "[Epoch 15, Batch 200] loss: 0.001\n",
      "[Epoch 15, Batch 300] loss: 0.001\n",
      "[Epoch 15, Batch 400] loss: 0.001\n",
      "[Epoch 15, Batch 500] loss: 0.001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):  \n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs.permute(0, 3, 1, 2))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "torch.save(net.state_dict(), 'model-a.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.23\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images.permute(0, 3, 1, 2))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on trigger-A test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.51\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testA_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images.permute(0, 3, 1, 2))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on trigger-B test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.81\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testB_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images.permute(0, 3, 1, 2))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the trained model has 74.23% accuracy on the normal test set, which is a good performance in normal scenarios. It reaches 96.51% and 98.81% accuracy on the A and B test sets respectively, proving that the backdoor has a very good effect on this model. The poisoning attack is successful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
